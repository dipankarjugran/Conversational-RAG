{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain faiss-cpu sentence-transformers google-generativeai langchain-community python-dotenv langchain-openai"
      ],
      "metadata": {
        "id": "mpvKFAXDpI8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0aQLLDCpEh9"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "import google.generativeai as genai\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Read the entire file as a single string\n",
        "file_path = \"/content/bugreport-lynx-BP2A.250605.031.A2-2025-06-24-13-51-27.txt\"\n",
        "with open(file_path, \"r\", encoding=\"latin-1\") as file:\n",
        "    full_text = file.read()\n",
        "\n",
        "# Initialize the splitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=5000,          # Adjust as needed\n",
        "    chunk_overlap=500,        # Adjust based on how much context you want retained\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \"]  # Split by paragraphs, then lines, then sentences\n",
        ")\n",
        "\n",
        "# Create document chunks\n",
        "split_texts = text_splitter.split_text(full_text)\n",
        "document_based_chunks = [Document(page_content=chunk, metadata={}) for chunk in split_texts]\n",
        "\n",
        "# Print number of chunks and optionally one sample\n",
        "print(f\"Number of chunks: {len(document_based_chunks)}\")\n",
        "print(f\"First chunk preview:\\n{document_based_chunks[0].page_content[:500]}\")\n",
        "\n",
        "# Populate FAISS\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "vector_store = FAISS.from_documents(document_based_chunks, embedding_model)\n",
        "# Save FAISS index and document mapping\n",
        "vector_store.save_local(\"faiss_index_adb\")\n",
        "vector_store_adb = FAISS.load_local(\"faiss_index_adb\", embedding_model, allow_dangerous_deserialization = True)"
      ]
    }
  ]
}